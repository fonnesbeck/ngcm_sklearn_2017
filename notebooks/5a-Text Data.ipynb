{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section draws heavily from the [official scikit-learn tutorial on text classification](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Working with text data is a particularly attractive use case for machine learning. It's also often a messy one that can involve working with a lot of boilerplate code. Scikit-Learn provides many features for working with text data.\n",
    "\n",
    "In this section, we're going to work with the canonical \"20 newsgroups\" data set.\n",
    "\n",
    "Newsgroups are like reddit before reddit.\n",
    "\n",
    "From the [web site](http://qwone.com/~jason/20Newsgroups/)\n",
    "\n",
    "\n",
    "> The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training portion of the dataset has been downloaded, stored on your machine, and made available in memory via `sklearn.datasets.load_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we'll want to build a classifier on these names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to take our text and turn it in to numerical features. A common assumption for doing machine learning on text is what's known as the bag of words assumption. This means that we assume that the order of the words as they occur in a document doesn't matter to discern tje general meaning of the document. This is commonly done in the following steps\n",
    "\n",
    "1. Build what's called a *vocabulary*, which is a mapping from integers to possible words, $w$, in your *corpus*, or collection of documents.\n",
    "2. Using this *vocabulary*, assign a number to the count of each word occuring in any document.\n",
    "\n",
    "What you're left with is a matrix $X$, where each value $X[i,j]$ is the count of word $j$ in document $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if your vocabulary is `[computer, protest, array]`\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ is a matrix of dimension `n_documents` by `n_vocabulary`. This is large. Luckily, most words don't occur in every document. If they did, we would not be able to separate the documents according to topics.\n",
    "\n",
    "Bag of words documents are often referred to as high-dimensional, sparse datasets. We don't need to keep the zeros in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so how do we do this? Text is often really messy, has punctuation, and has a bunch of words that every text has to have but don't necessarily connote topical meaning. These words are called *stop words* such as \"the,\" \"a,\" or \"an.\"\n",
    "\n",
    "We turn human writing into a set of feature vectors by taking care of these issues. This process is called *tokenization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides some nice facilities for building a dictionary of features and transform documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vectorizer.fit_transform(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.prod(X_train_counts.shape) / (8 * 1000 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained `CountVectorizer` transformer has a `vocabulary_` attribute that's a dictionary of feature indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_.get('algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = count_vectorizer.vocabulary_.get('algorithm')\n",
    "X_train_counts[:, idx].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurrences to Frequencies\n",
    "\n",
    "There's one issue so far. Number of occurrences is correlated with document length. Instead, we should look at the *term frequency*. This is the frequency of occurences of a word in a document. Term frequency in document $i$ for word $j$ is\n",
    "\n",
    "$$tf_{ij}=\\frac{w_{ij}}{\\sum_jw_{ij}}$$\n",
    "\n",
    "You might go about computing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "tf = normalize(X_train_counts, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[:3, :].sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great. What's the most frequently used term in one of the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx = tf[1234, :].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a reverse mapping using `get_feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names()[word_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important concept is that of *inverse document frequency*. This is a measure of how important a word is. Words like stop words or words that are otherwise popular in a corpus will still have a high term frequency. Inverse document frequency is a way to downweight the frequent terms but upweight the rare ones. The inverse document frequency is\n",
    "\n",
    "$$idf = \\log\\left(\\frac{N_{\\text{documents}}}{N_{\\text{documents with term}}}\\right)$$\n",
    "\n",
    "You'll often see\n",
    "\n",
    "$$idf = \\log\\left(\\frac{N_{\\text{documents}}}{1 + N_{\\text{documents with term}}}\\right)$$\n",
    "\n",
    "In case your vocabulary is a superset of the words in your documents.\n",
    "\n",
    "So tf-idf is\n",
    "\n",
    "$$\\text{tf-idf} = tf * idf$$\n",
    "\n",
    "Scikit-learn actually uses a *slightly* different definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, scikit-learn provides a transformer for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a classifier. One of the first off-the-shelf, textbook classifiers for text documents is the Naive Bayes classifier. Naive Bayes is so named because it applies Bayes' theorem to classify samples, relying on a naive (incorrect) assumption on the independence of features. \n",
    "\n",
    "We estimate the probability of a record with $k$ features  $\\boldsymbol{w}=(w_1,\\dots,w_k)$ belonging to some class $C_j$ for $J$ potential classes as\n",
    "\n",
    "$$p(C_j\\mid\\boldsymbol{w}) = \\frac{p(C_j)p(\\boldsymbol{w} \\mid C_j)}{p(\\boldsymbol{w})}$$\n",
    "\n",
    "Thus, the probability of $w_i$ belonging to $C_j$ is based on our prior belief for the incidence of $C_j$, the likelihood of observing the data that we did, $\\boldsymbol{w}$, given that the true class is $C_j$, and the evidence $p(w)$, which is constant and ignored.\n",
    "\n",
    "We are, therefore, interested in the numerator only whose joint probability model is\n",
    "\n",
    "$$p(C_k,w_1,\\dots,w_k)$$\n",
    "\n",
    "We want to select the class $C_k$ that maximizes this probability. By assuming that the features are independent, this probability is equivalent to\n",
    "\n",
    "$$p(C_k)\\prod_{i=1}^k p(w_i\\mid C_k)$$\n",
    "\n",
    "so the classifier is the solution to\n",
    "\n",
    "$$\\hat{y} = \\underset{j \\in \\{1,\\dots,J\\}}{\\text{argmax}}p(C_j)\\prod_{i=1}^k p(w_i \\mid C_j)$$\n",
    "\n",
    "For text documents, we rely on the bag of words assumption and, thus, the event probability model is multinomial. The frequency of words in a document are generated via a multinomial distribution with parameter $\\boldsymbol{p} = (p_1,\\dots,p_k)$. As per usual, we estimate the log-likelihood\n",
    "\n",
    "$$\\hat{y} = \\underset{j \\in \\{1,\\dots,J\\}}{\\text{argmax}}\\log p(C_j) + \\sum_{i=1}^k \\log p(w_i \\mid C_j)$$\n",
    "\n",
    "where the maximum liklelihood estimate for the prior $p(C_j)$ is simple the relative frequencies that we observe for each class and, similarly, the MLE for $p(w_i \\mid C_j)$ are the relative frequencies of each term in each class. We often use Laplace smoothing to avoid the problem that rare occurrences of words will not appear in the training data. So instead of calculating the raw relative frequencies we assume a uniform prior on all words and add one to their occurrences when counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_tfidf, newsgroups.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    'What kind of car is this',\n",
    "    'This GPU and RAM in my new computer is awesome',\n",
    "    'ESA is doing some cool things',\n",
    "    'This old thing is for sale'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = count_vectorizer.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf_new = tfidf.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_tfidf_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, category in zip(docs, predictions):\n",
    "    target_name = newsgroups.target_names[category]\n",
    "    print(f\"{doc}: {target_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the above in to a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the test set using `fetch_20newsgroups`. Use a few different classifiers and evaluate them vs. the test set.\n",
    "\n",
    "TODO: suggest some libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Beyond "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of great libraries for working with text data in Python. Two very popular ones are\n",
    "\n",
    "* [NLTK](http://www.nltk.org/)\n",
    "* [gensim](https://radimrehurek.com/gensim/)\n",
    "\n",
    "But there are [many more](https://github.com/keon/awesome-nlp#user-content-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning \n",
    "\n",
    "We should point out that much of modern NLP and text modeling takes advantage of advances in [Deep Learning](https://github.com/keon/awesome-nlp#deep-learning-for-nlp) that allow estimators to go beyond the bag of words assumption. There are a few examples in the [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
